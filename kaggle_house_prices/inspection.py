# AUTOGENERATED! DO NOT EDIT! File to edit: notebooks/03_model_inspection.ipynb (unless otherwise specified).

__all__ = ['get_df_preds', 'distribution_similarity', 'element_comp', 'get_categorical_freqs_single_col',
           'plot_best_vs_worst_categorical', 'get_all_categorical_freqs', 'plot_top_distributions',
           'get_continuous_freqs_single_col', 'plot_best_vs_worst_continuous', 'get_all_continuous_freqs',
           'FastaiModel']

# Cell
import pandas as pd
from fastai2.tabular.all import *
from fastai2 import metrics
from sklearn import ensemble
import pickle
from .utils import *
from .preprocessing import *
from .modelling import *
from typing import List, Union
import matplotlib.pyplot as plt
from scipy import stats
import shap

# Cell
def get_df_preds(df:pd.DataFrame, learn:Learner, cat_names:List[str], cont_names:List[str]) -> torch.Tensor:
    "`df` is the `TabularPandas.valid.xs` attribute"
    x_cat = torch.from_numpy(df[cat_names].values).long()
    x_cont = torch.from_numpy(df[cont_names].values).float()
    learn.model.eval()
    return learn.model.forward(x_cat, x_cont)

# Cell
def distribution_similarity(best:pd.Series, worst:pd.Series):
    "Computing distribution similarity using `stats.wassertain_distance` assuming index based order"
    if len(worst.values) == 0 or len(best.values) == 0: return np.inf
    return stats.wasserstein_distance(best.values, worst.values)

# Cell
def element_comp(x:np.ndarray, y:Union[int,float,str]) -> np.ndarray:
    "Element-wise comparison"
    return np.array([v == y for v in x])

def get_categorical_freqs_single_col(col:str, df:pd.DataFrame, best:np.ndarray, worst:np.ndarray):
    "Computing the frequencies of categorical variables"
    _best = df.iloc[best][col].value_counts(normalize=True)
    _worst = df.iloc[worst][col].value_counts(normalize=True)
    m = [v for v in _best.index if v not in _worst.index]
    if len(m)>0: _worst.append(pd.Series(data=[0 for _ in m], index=m), ignore_index=True)
    m = [v for v in _worst.index if v not in _best.index]
    if len(m)>0: _best.append(pd.Series(data=[0 for _ in m], index=m), ignore_index=True)

    _best_nas = 0 if "#na#" not in _best.index else _best.pop("#na#")
    _worst_nas = 0 if "#na#" not in _worst.index else _worst.pop("#na#")
    _best.sort_index(inplace=True)
    _worst.sort_index(inplace=True)
    return _best, _worst, _best_nas, _worst_nas

def plot_best_vs_worst_categorical(col:str, best:pd.Series,
                                   worst:pd.Series, best_nas:int=0,
                                   worst_nas:int=0,w:float=.4):
    "Plotting best/worst distribution for a categorical variable"
    if len(best) == 0:
        print(f"{col} has no finite values among the 'best' chosen samples!")
        return None
    if len(worst) == 0:
        print(f"{col} has no finite values among the 'worst' chosen samples!")
        return None

    fig, ax = plt.subplots()
    df_b = best.to_frame(name="best").reset_index().rename(columns={"index":col})
    df_w = worst.to_frame(name="worst").reset_index().rename(columns={"index":col})

    df = df_b.merge(df_w, on=col, how="outer")
    df.plot(kind="bar", x=col, ax=ax)

    ax.set_xlabel(col)
    ax.set_ylabel("Category frequency")
    ax.set_title(f"Frequecies of '{col}': best ({int(best_nas):d} nas) vs worst ({int(worst_nas):d} nas) loss", fontsize=16)
    ax.legend(title="Set")
    plt.show()

# Cell
def get_all_categorical_freqs(decoded_xs:pd.DataFrame, best:np.ndarray, worst:np.ndarray):
    "Computes value frequencies for each categorical feature for both best and worst predictions."
    cat_freqs = {}
    for col in to.cat_names:
        _best, _worst, _best_nas, _worst_nas = get_categorical_freqs_single_col(col, decoded_xs, best, worst)
        _score = distribution_similarity(_best, _worst)
        cat_freqs[col] = {"best": _best, "worst":_worst, "score": _score,
                          "best_nas": _best_nas, "worst_nas": _worst_nas}
    return cat_freqs

# Cell
def plot_top_distributions(cat_freqs:dict=None, cont_freqs:dict=None, top_n:int=5):
    "Calling `plot_best_vs_worst_categorical` and `plot_best_vs_worst_continuous` showing distributions with the worst scores"
    all_freqs = {"cat": cat_freqs, "cont": cont_freqs}
    for kind, freqs in all_freqs.items():
        if freqs is None: continue
        for i, col in enumerate(sorted(freqs, key=lambda x: freqs[x]["score"], reverse=True)):
            if i > top_n: break
            if kind == "cat": plot_best_vs_worst_categorical(col,
                                                             freqs[col]["best"],
                                                             freqs[col]["worst"],
                                                             best_nas=freqs[col]["best_nas"],
                                                             worst_nas=freqs[col]["worst_nas"])
            if kind == "cont": plot_best_vs_worst_continuous(col,
                                                             freqs[col]["best"],
                                                             freqs[col]["worst"],
                                                             best_nas=freqs[col]["best_nas"],
                                                             worst_nas=freqs[col]["worst_nas"])

# Cell
def get_continuous_freqs_single_col(col:str, df:pd.DataFrame, best:np.ndarray, worst:np.ndarray,
                   nbins:int=50):
    "Computing the frequencies of continuous variables"
    _best = df.iloc[best][col].values
    _best_nas = element_comp(_best, "#na#")
    _best = _best[~_best_nas]
    _best_nas = _best_nas.sum()

    _worst = df.iloc[worst][col].values
    _worst_nas = element_comp(_worst, "#na#")
    _worst = _worst[~_worst_nas]
    _worst_nas = _worst_nas.sum()

    lim = (min(_best.min(), _worst.min()),
           max(_best.max(), _worst.max()))
    bins = np.linspace(lim[0], lim[1], nbins+1)
    _best, _ = np.histogram(_best, bins=bins)
    _worst, _ = np.histogram(_worst, bins=bins)
    centers = .5*(bins[1:] + bins[:-1])
    _best = pd.Series(index=centers, data=_best)
    _worst = pd.Series(index=centers, data=_worst)
    return _best, _worst, _best_nas, _worst_nas

def plot_best_vs_worst_continuous(col:str, best:pd.Series,
                                  worst:pd.Series, best_nas:int=0,
                                  worst_nas:int=0):
    "Plotting best/worst distribution for a continuous variable"
    fig, ax = plt.subplots()

    assert np.allclose(best.index, worst.index)
    w = best.index[1] - best.index[0]  # assumes regular grid
    ax.bar(best.index-w/2, best.values, label="best", alpha=.9, width=w)
    ax.bar(worst.index+w/2, worst.values, label="worst", alpha=.9, width=w)

    ax.set_xlabel(col)
    ax.set_ylabel("Continuous value frequency")
    ax.set_title(f"Frequecies of '{col}': best ({best_nas:d} nas) vs worst ({worst_nas:d} nas) loss", fontsize=16)
    ax.legend(title="Set")
    plt.show()

def get_all_continuous_freqs(decoded_xs:pd.DataFrame, best:np.ndarray, worst:np.ndarray):
    cont_freqs = {}
    for col in to.cont_names:
        _best, _worst, _best_nas, _worst_nas = get_continuous_freqs_single_col(col, decoded_xs, best, worst, nbins=50)
        _score = distribution_similarity(_best, _worst)
        cont_freqs[col] = {"best": _best, "worst":_worst, "score": _score,
                           "best_nas": _best_nas, "worst_nas": _worst_nas}

    return cont_freqs

# Cell
class FastaiModel(torch.nn.Module):
    "Wrapper class to make `TabularModel` usable with `shap.DeepExplainer`"
    def __init__(self, learn:Learner, to:TabularPandas):
        super().__init__()
        self.cat_ix = [_i for _i,_v in enumerate(to.valid.xs.columns) if _v in to.cat_names]
        self.cont_ix = [_i for _i,_v in enumerate(to.valid.xs.columns) if _v in to.cont_names]
        self.learn = learn

    def forward(self, X):
        return self.learn.model(X[:,self.cat_ix].long(), X[:,self.cont_ix].float())