# AUTOGENERATED! DO NOT EDIT! File to edit: notebooks/03_model_inspection.ipynb (unless otherwise specified).

__all__ = ['get_df_preds', 'element_comp', 'get_cat_freqs', 'plot_best_vs_worst_categorical', 'distribution_similarity',
           'get_cont_freqs', 'plot_best_vs_worst_continuous']

# Cell
import pandas as pd
from fastai2.tabular.all import *
from fastai2 import metrics
from sklearn import ensemble
import pickle
from .utils import *
from .preprocessing import *
from .modelling import *
from typing import List, Union
import matplotlib.pyplot as plt
from scipy import stats

# Cell
def get_df_preds(df:pd.DataFrame, learn:Learner, cat_names:List[str], cont_names:List[str]) -> torch.Tensor:
    "`df` is the `TabularPandas.valid.xs` attribute"
    x_cat = torch.from_numpy(df[cat_names].values).long()
    x_cont = torch.from_numpy(df[cont_names].values).float()
    learn.model.eval()
    return learn.model.forward(x_cat, x_cont)

# Cell
def element_comp(x:np.ndarray, y:Union[int,float,str]) -> np.ndarray:
    return np.array([v == y for v in x])

def get_cat_freqs(col:str, df:pd.DataFrame, best:np.ndarray, worst:np.ndarray):
    "Computing the frequencies of categorical variables"
    _best = df.iloc[best][col].value_counts(normalize=True)
    _worst = df.iloc[worst][col].value_counts(normalize=True)
    m = [v for v in _best.index if v not in _worst.index]
    if len(m)>0: _worst.append(pd.Series(data=[0 for _ in m], index=m), ignore_index=True)
    m = [v for v in _worst.index if v not in _best.index]
    if len(m)>0: _best.append(pd.Series(data=[0 for _ in m], index=m), ignore_index=True)

    _best_nas = 0 if "#na#" not in _best.index else _best.pop("#na#")
    _worst_nas = 0 if "#na#" not in _worst.index else _worst.pop("#na#")
    _best.sort_index(inplace=True)
    _worst.sort_index(inplace=True)
    return _best, _worst, _best_nas, _worst_nas

def plot_best_vs_worst_categorical(col:str, best:pd.Series,
                                   worst:pd.Series, best_nas:int=0,
                                   worst_nas:int=0,w:float=.4):
    "Plotting best/worst distribution for a categorical variable"
    if len(best) == 0:
        print(f"{col} has no finite values among the 'best' chosen samples!")
        return None
    if len(worst) == 0:
        print(f"{col} has no finite values among the 'worst' chosen samples!")
        return None

    fig, ax = plt.subplots()
    df_b = best.to_frame(name="best").reset_index().rename(columns={"index":col})
    df_w = worst.to_frame(name="worst").reset_index().rename(columns={"index":col})

    df = df_b.merge(df_w, on=col, how="outer")
    df.plot(kind="bar", x=col, ax=ax)

    ax.set_xlabel(col)
    ax.set_ylabel("Category frequency")
    ax.set_title(f"Frequecies of '{col}': best ({best_nas:d} nas) vs worst ({worst_nas:d} nas) loss", fontsize=16)
    ax.legend(title="Set")
    plt.show()

def distribution_similarity(best:pd.Series, worst:pd.Series):
    "Computing distribution similarity using `stats.wassertain_distance`"
    if len(worst.values) == 0 or len(best.values) == 0: return np.inf
    return stats.wasserstein_distance(best.values, worst.values)

def get_cont_freqs(col:str, df:pd.DataFrame, best:np.ndarray, worst:np.ndarray,
                   nbins:int=50):
    "Computing the frequencies of continuous variables"
    _best = df.iloc[best][col].values
    _best_nas = element_comp(_best, "#na#")
    _best = _best[~_best_nas]
    _best_nas = _best_nas.sum()

    _worst = df.iloc[worst][col].values
    _worst_nas = element_comp(_worst, "#na#")
    _worst = _worst[~_worst_nas]
    _worst_nas = _worst_nas.sum()

    lim = (min(_best.min(), _worst.min()),
           max(_best.max(), _worst.max()))
    bins = np.linspace(lim[0], lim[1], nbins+1)
    _best, _ = np.histogram(_best, bins=bins)
    _worst, _ = np.histogram(_worst, bins=bins)
    centers = .5*(bins[1:] + bins[:-1])
    _best = pd.Series(index=centers, data=_best)
    _worst = pd.Series(index=centers, data=_worst)
    return _best, _worst, _best_nas, _worst_nas

def plot_best_vs_worst_continuous(col:str, best:pd.Series,
                                  worst:pd.Series, best_nas:int=0,
                                  worst_nas:int=0):
    "Plotting best/worst distribution for a continuous variable"
    fig, ax = plt.subplots()

    assert np.allclose(best.index, worst.index)
    w = best.index[1] - best.index[0]  # assumes regular grid
    ax.bar(best.index-w/2, best.values, label="best", alpha=.5, width=w)
    ax.bar(worst.index+w/2, worst.values, label="worst", alpha=.5, width=w)

    ax.set_xlabel(col)
    ax.set_ylabel("Continuous value frequency")
    ax.set_title(f"Frequecies of '{col}': best ({best_nas:d} nas) vs worst ({worst_nas:d} nas) loss", fontsize=16)
    ax.legend(title="Set")
    plt.show()